{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'train_test_spilt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1ed0629b3cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_spilt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'train_test_spilt'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_spilt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(file_path):\n",
    "    tokenizer =Tokenizer(filters=\"\")\n",
    "    whole_texts=[]\n",
    "    for line in open(file_path,encoding='utf-8'):\n",
    "        #<s >< /s>←空白が必要\n",
    "        whole_texts.append(\"<s> \"+line.strip()+\" </s>\")\n",
    "        \n",
    "    tokenizer.fit_on_texts(whole_texts)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts),tokenizer\n",
    "\n",
    "x_train,tokenizer_en = load_data('data/train.en')\n",
    "y_train,tokenizer_ja = load_data('data/train.ja')\n",
    "\n",
    "en_vocab_size=len(tokenizer_en.word_index)+1\n",
    "ja_vocab_size=len(tokenizer_ja.word_index)+1\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.02,random_state=42)\n",
    "x_train=pad_sequences(x_train,padding='post')\n",
    "y_train=pad_sequences(y_train,padding='post')\n",
    "\n",
    "seqX_len=len(x_train[0])\n",
    "seqY_len=len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.kayers import Input,Embedding,Dence,LSTM\n",
    "\n",
    "emb_dim=256\n",
    "hid_dim=256\n",
    "##符号化器\n",
    "#Inputレイヤー（返り値としてテンソルを受け取る）\n",
    "encoder_inputs=Input(shape=(seqX_len,))#(seqX_len,)でseqX_lenという要素だけ持つTupleとなる\n",
    "\n",
    "#モデルの層構成(手前の層の返り値テンソルを、次の接続したい層に別途引数として与える)\n",
    "encoder_embedded=Embedding(en_vocab_size,emb_dim,mask_zero=True)(encoder_inputs)# shape: (seqX_len,)->(seqX_len, emb_dim)\n",
    "# EmbeddingレイヤーとLSTMレイヤーを接続（+LSTMレイヤーのインスタンス化）\n",
    "#*はunpacedある配列i=[1,2,3]をprint('i[1],i[2],i[3]')とできる(多分、zipと一緒に使ったよね)\n",
    "_,*encoder_states=LSTM(hid_dim,return_state=True)(encoder_embedded)# shape: (seqX_len, emb_dim)->(hid_dim, )\n",
    "# このLSTMレイヤーの出力に関しては下記に補足あり(output(h_t),state_h(隠れ状態),state_c(セル状態)=LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##複合化器\n",
    "#Inputレイヤー(返り値としてテンソルを受け取る)\n",
    "decoder_inputs=Input(shape=(seqY_len))\n",
    "\n",
    "#モデルの層構成(手前の層の返り値テンソルを、次の接続したい層に別途引数として与える)\n",
    "decoder_embedding=Embedding(ja_vocab_size,emb_dim)\n",
    "decoder_embedded=decoder_embedding(decoder_inputs)\n",
    "#EmbeddingレイヤーとLSTMレイヤーを接続(encoder_statesを初期状態として指定)\n",
    "decoder_lstm=LSTM(hid_dim,return_sequences=True,return_state=True)#あとで参照したいので、レイヤー自体を変数化\n",
    "#(_は今回はoutputはいらないため受け取らない)\n",
    "decoder_outputs,_,_=decoder_lstm(decoder_embedding,initial_state=encoder_states)#shape:(seqY_len,emb_dim)->(seqY_len,hid_dim)\n",
    "#LSTMレイヤーとDenseレイヤーを接続\n",
    "decoder_dense=Dense(ja_vocab_size,activation='softmax')\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "\n",
    "model=Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compilr(optimizer='rmsprop',loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#サンプリング用（生成用）のモデルを作成\n",
    "\n",
    "#符号化器(学習時と同じ構成、学習したレイヤーを利用)\n",
    "encoder_model=Model(encoder_inputs,encoder_states)\n",
    "\n",
    "#複合化器\n",
    "decoder_states_inputs=[Input(shape=(hid_dim,)),Input(shape=(hid_dim,))]#decorder_lstmの初期状態指定用(h_t,c_t)\n",
    "\n",
    "\n",
    "decoder_inputs=Input(shape=(1,))\n",
    "decoder_embedded=decoder_embedding(decoder_imputs)#学習済みEmbeddingレイヤーを利用\n",
    "decoder_outputs,*decoder_states=decoder_lstm(decoder_embedding,initial_state=decoder_states_inputs)#学習済みLSTMレイヤーを利用\n",
    "decoder_outputs,*decoder_dense(decoder_outputs)#学習済みDenseレイヤーを利用\n",
    "\n",
    "decoder_model=Model([decoder_input]+decoder_states_inputs,\\\n",
    "                    [decoder_out_puts]+decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq,bos_eos,max_output_length=1000):\n",
    "    states_value=encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq=np.array(bos_eos[0])\n",
    "    output_seq=bos_eos[0][:]\n",
    "    \n",
    "    while True:\n",
    "        output_tokens,*states_value=decoder_model.predict([target_seq]+states_value)\n",
    "        sampled_token_index=[np.argmax(output_tokens[0,-1,:])]\n",
    "        output_seq+=sampled_token_index\n",
    "        \n",
    "        if(sampled_token_index == bos_eos[1] or len(output_seq)>max_output_length):\n",
    "            break\n",
    "            \n",
    "        target_seq=np.array(sampled_token_index)\n",
    "        \n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer_en=dict(map(reversed,tokenizer_en.word_index.items()))\n",
    "detokenizer_ja=dict(map(reversed,tokenizer_ja.word_index.items()))\n",
    "\n",
    "text_no=0\n",
    "input_seq=pad_sequences([x_test[text_no]],seqX_len,padding=\"post\")\n",
    "bos_eos=tokenizer_ja.texts_to_sequences([\"<s>\",\"</s>\"])\n",
    "\n",
    "print('元の文',''.join([detokenizer_en[i] for i in x_test[text_no]]))\n",
    "print('生成文',''.join([detokenizer_ja[i] for i in y_test[text_no]]))\n",
    "print('正解文',''.join([detokenizer_ja[i] for i in y_test[test_no]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer_en=dict(map(reversed,tokenizer_en.word_index.items()))\n",
    "detokenizer_ja=dict(map(reversed,tokenizer_ja.word_index.items()))\n",
    "\n",
    "text_no=0\n",
    "input_seq=pad_sequences([x_test[text_no]],seqX_len,padding=\"post\")\n",
    "bos_eos=tokenizer_ja.texts_to_sequences([\"<s>\",\"</s>\"])\n",
    "\n",
    "print('元の文',''.join([detokenizer_en[i] for i in x_test[text_no]]))\n",
    "print('生成文',''.join([detokenizer_ja[i] for i in y_test[text_no]]))\n",
    "print('正解文',''.join([detokenizer_ja[i] for i in y_test[test_no]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "\n",
    "prediction=['I','am','a','graduate','student','at','a','university']\n",
    "reference=[['I','am','a','graduate','student','at','the','university','of','tokyo']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no =1\n",
    "input_seq=pad_sequences([x_test[test_no]],seqX_len,padding='post')\n",
    "bos_eos=tokenizer_ja.texts_to_sequences([\"<s>\",\"</s>\"])\n",
    "\n",
    "prediction=[detokenizer_ja[i] for i in decode_sequence(input_seq,bos_eos)]\n",
    "\n",
    "reference=[[derokenizer_ja[i] for i in y_test[text_no]]]\n",
    "\n",
    "print(prediction)\n",
    "print(reference)\n",
    "\n",
    "print(sentence_bleu(reference,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ここからattention付き'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(file_path):#filepathでデータのダウンロード\n",
    "    tokenizer=Tokenizer(filters=\"\")\n",
    "    whole_texts=[]\n",
    "    for line in open(file_path,encoding='utf-8'):\n",
    "        whole_texts.append(\"<s> \"+line.strip()+\" </s>\")#文の始まりと終わりの指定\n",
    "        \n",
    "    tokenizer.fit_on_texts(whole_texts)#今回はバイナリベクトル表現化している\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts),tokenizer\n",
    "#読み込みとtokenizerによる数値化\n",
    "x_train,tokenizer_en=load_data('data/train.en')\n",
    "y_train,tokenizer_ja=load_data('data/train.ja')\n",
    "\n",
    "en_vocab_size=len(tokenizer_en.word_index)+1\n",
    "ja_vocab_size=len(tokenizer_ja.word_index)+1\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.02,random_state=42)\n",
    "\n",
    "x_train=pad_sequences(x_train,padding='post')#各シーケンスの後をパディングする\n",
    "y_train=pad_sequences(y_train,padding='post')\n",
    "\n",
    "seqX_len=len(x_train[0])\n",
    "seqY_len=len(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Permute,Activation,Embedding,Dense,LSTM,concatenate,dot\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "emb_dim=256\n",
    "hid_dim=256\n",
    "att_dim=256\n",
    "\n",
    "#符号化器\n",
    "encoder_inputs = Input(shape=(seqX_len,))#英語文の一番長い文章の長さでshape\n",
    "#mask_zeroはパディング部分を無視する\n",
    "encoder_embedded=Embedding(en_vocab_size,emb_dim,mask_zero=True)(encoder_inputs)#単語ごとに入力\n",
    "\n",
    "encoded_seq,*encoder_states=LSTM(hid_dim,return_sequences=True,return_state=True)(encoder_embedded)\n",
    "\n",
    "#復号化器\n",
    "decoder_inputs=Input(shape=(seqY_len,))#日本語の一番長い文でshape\n",
    "decoder_embedding=Embedding(ja_vocab_size,emb_dim)#あとで参照するためにレイヤー自体の変数化を行う\n",
    "decoder_embedded=decoder_embedding(decoder_inputs)\n",
    "decoder_lstm=LSTM(hid_dim,return_sequences=True,return_state=True)\n",
    "decoded_seq,_,_=decoder_lstm(decoder_embedded,initial_state=encoder_states)\n",
    "\n",
    "#Attention\n",
    "score_dense=Dense(hid_dim)#全結合層の変数化\n",
    "score=score_dense(decoded_seq)#デコーダの隠れ層出力を全結合層へ\n",
    "\n",
    "score=dot([score,encoded_seq],axes=(2,2))#エンコーダとの出力との類似度を内積でとる\n",
    "\n",
    "attention=Activation('softmax')(score)#正規化\n",
    "\n",
    "context=dot([attention,encoded_seq],axes=(2,1))#アテンションの値でエンコーダの値を重み付け\n",
    "\n",
    "concat=concatenate([context,decoded_seq],axis=2)#アテンションとデコーダの連結(行で並べる)\n",
    "\n",
    "attention_dense=Dense(att_dim,activation='tanh')#attentionの全結合層\n",
    "attentional=attention_dense(concat)#concatをinput\n",
    "\n",
    "output_dense=Dense(ja_vocab_size,activation='softmax')\n",
    "outputs=output_dense(attentional)#outputはアテンションを適応したもの\n",
    "\n",
    "model=Model([encoder_inputs,decoder_inputs],outputs)\n",
    "model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39200 samples, validate on 9800 samples\n",
      "Epoch 1/10\n",
      " - 26s - loss: 2.9412 - val_loss: 2.3713\n",
      "Epoch 2/10\n",
      " - 24s - loss: 2.1379 - val_loss: 2.0467\n",
      "Epoch 3/10\n",
      " - 24s - loss: 1.9104 - val_loss: 1.8904\n",
      "Epoch 4/10\n",
      " - 24s - loss: 1.7688 - val_loss: 1.8026\n",
      "Epoch 5/10\n",
      " - 24s - loss: 1.6553 - val_loss: 1.6781\n",
      "Epoch 6/10\n",
      " - 24s - loss: 1.5542 - val_loss: 1.6319\n",
      "Epoch 7/10\n",
      " - 24s - loss: 1.4598 - val_loss: 1.5375\n",
      "Epoch 8/10\n",
      " - 24s - loss: 1.3679 - val_loss: 1.4805\n",
      "Epoch 9/10\n",
      " - 24s - loss: 1.2803 - val_loss: 1.4076\n",
      "Epoch 10/10\n",
      " - 24s - loss: 1.1997 - val_loss: 1.3556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f350d76add8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_target=np.hstack((y_train[:,1:],np.zeros((len(y_train),1),dtype=np.int32)))\n",
    "\n",
    "model.fit([x_train,y_train],np.expand_dims(train_target,-1),batch_size=128,epochs=10,verbose=2,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成のフェイズ\n",
    "#学習したエンコーダレイヤーを利用(encoder_inputやencoded_seq,encoder_states)\n",
    "encoder_model=Model(encoder_inputs,[encoded_seq]+encoder_states)\n",
    "#学習したエンコーダに合わせてimputの設定(バッチサイズhid_dim)\n",
    "decoder_states_inputs=[Input(shape=(hid_dim,)),Input(shape=(hid_dim,))]\n",
    "#デコーダのインプットは一次元\n",
    "decoder_inputs=Input(shape=(1,))\n",
    "#学習済みモデルで構築\n",
    "decoder_embedded=decoder_embedding(decoder_inputs)\n",
    "#学習済みモデルで構築(initialstateは内部状態の初期値(今回は(hid_dim,hid_dim)))\n",
    "decoded_seq,*decoder_states=decoder_lstm(decoder_embedded,initial_state=decoder_states_inputs)\n",
    "#inputが第一引数、アウトプットが第二引数\n",
    "decoder_model=Model([decoder_inputs]+decoder_states_inputs,[decoded_seq]+decoder_states)\n",
    "#バッチサイズseqX_len,inputdim=hid_dim\n",
    "encoded_seq_in,decoded_seq_in=Input(shape=(seqX_len,hid_dim)),Input(shape=(1,hid_dim))\n",
    "#学習済みdense層を用いる\n",
    "score=score_dense(decoded_seq_in)\n",
    "#以下学習済みattentionを用いる\n",
    "score=dot([score,encoded_seq_in],axes=(2,2))\n",
    "attention=Activation('softmax')(score)\n",
    "context=dot([attention,encoded_seq_in],axes=(2,1))\n",
    "concat=concatenate([context,decoded_seq_in],axis=2)\n",
    "attentional=attention_dense(concat)\n",
    "attention_outputs=output_dense(attentional)\n",
    "\n",
    "attention_model=Model([encoded_seq_in,decoded_seq_in],[attention_outputs,attention])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq,bos_eos,max_output_length=1000):\n",
    "    #predictで予測できるinput_seqは与えるエンコーダの予測がencoded_seqに入り、\n",
    "    #states_valueには時間ごとの(単語ごと)の隠れこみ層の値が入る\n",
    "    encoded_seq,*states_value=encoder_model.predict(input_seq)\n",
    "    print(bos_eos)\n",
    "    target_seq=np.array(bos_eos[0])#bos_eos[0]=\"<s>\"に対応するインデックス\n",
    "    output_seq = bos_eos[0][:]#終わりは</s>\n",
    "    #emptyでランダムな配列にできる\n",
    "    attention_seq=np.empty((0,len(input_seq[0])))\n",
    "    \n",
    "    while True:\n",
    "        #デコーダの予測値を格納\n",
    "        decoded_seq,*states_value=decoder_model.predict([target_seq]+states_value)\n",
    "        #アテンションの予測値を格納\n",
    "        output_token,attention=attention_model.predict([encoded_seq,decoded_seq])\n",
    "        #argmaxで今回追加したワードの中でもっとも数値が大きいものを選択(argmax内の処理は以下)\n",
    "        '''\n",
    "        import numpy as np\n",
    "        a=np.array([[[2,3],[4,5]],[[6,7],[8,9]]])\n",
    "        print(a[0,-1,:])\n",
    "        output:[4 5]\n",
    "        '''\n",
    "        sampled_token_index=[np.argmax(output_token[0,-1,:])]\n",
    "        #出力に値を追加する\n",
    "        output_seq+=sampled_token_index\n",
    "        #\n",
    "        attention_seq=np.append(attention_seq,attention[0],axis=0)\n",
    "        \n",
    "        if(sampled_token_index == bos_eos[1] or len(output_seq)>max_output_length):\n",
    "            break\n",
    "        #\n",
    "        target_seq=np.array(sampled_token_index)\n",
    "        \n",
    "    return output_seq,attention_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1    7   93 2863   35  234   12  290    3    2    0    0    0    0\n",
      "     0    0    0    0]]\n",
      "[[1], [2]]\n",
      "元の文: <s> you may extend your stay in tokyo . </s>\n",
      "生成文: <s> 東京 に は お 茶 を 持 っ て い る 。 </s>\n",
      "正解文: <s> 東京 滞在 を 延ば し て も い い で す よ 。 </s>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#逆順でソート\n",
    "detokenizer_en=dict(map(reversed,tokenizer_en.word_index.items()))\n",
    "detokenizer_ja=dict(map(reversed,tokenizer_ja.word_index.items()))\n",
    "#一文目を予測している\n",
    "text_no=0\n",
    "#各シーケンスの後をパディングする(一番初めの要素を与える)\n",
    "input_seq=pad_sequences([x_test[text_no]],seqX_len,padding='post')\n",
    "print(input_seq)\n",
    "#<s >< /s>←空白が必要\n",
    "bos_eos=tokenizer_ja.texts_to_sequences([\"<s>\",\"</s>\"])\n",
    "\n",
    "output_seq,attention_seq=decode_sequence(input_seq,bos_eos)\n",
    "\n",
    "print('元の文:', ' '.join([detokenizer_en[i] for i in x_test[text_no]]))\n",
    "print('生成文:', ' '.join([detokenizer_ja[i] for i in output_seq]))\n",
    "print('正解文:', ' '.join([detokenizer_ja[i] for i in y_test[text_no]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1e8eaf054711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdetokenizer_ja\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdetokenizer_ja\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_no\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "prediction = [detokenizer_ja[i] for i in output_seq]\n",
    "reference = [[detokenizer_ja[i] for i in y_test[text_no]]]\n",
    "print(sentence_bleu(reference, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 26481 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 20140 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12395 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12399 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12362 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 33590 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12434 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 25345 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12387 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12390 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12356 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12427 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 12290 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 26481 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 20140 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12395 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12399 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12362 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 33590 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12434 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 25345 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12387 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12390 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12356 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12427 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n",
      "/home/udoncat/anaconda3/envs/tf-gpu/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py:176: RuntimeWarning: Glyph 12290 missing from current font.\n",
      "  font.load_char(ord(s), flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD+CAYAAAAeRj9FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAZRUlEQVR4nO3df7gdVX3v8ffnnBCI/EigB/xBfqEGIUUeaEMsYguWkAZ7Nb1XiuDlURSap/eWUqrSSx9ttKAtSK3FglaMQQu2FGnFaINQEWyN/EgICCSWmksVQigmiKmUn0m+/WPWDnMO55w9+2Sf2WtOPq8882TvPWuvtWZm7+9eZ83MWooIzMwsf329roCZmVXjgG1m1hAO2GZmDeGAbWbWEA7YZmYN4YBtZtYQDti7MUnTJP3fLuY3W9ID3cpvV0g6T9LLaizvu13Io+3xkHSCpK/valk5knS6pA/2uh45a3zAljRZ0t6jrN+/zvo0zDSgawF7PEnq7/At5wG1BeyIeGMXsmnM8eiGYb67i4BvVEy7W2pswJZ0uKRPAA8Ch6bXLpa0XtJ9kv4sJX2HpAckfUDSgR2WcZGk3ys9/5ik35N0acrzfknvSOsGtXwkXS7pzAplzJb0r5KWpTy/JGmBpFWSfiBpflq+K+me9P/r0nv/RdJRpbxWSTpS0hmS7pJ0r6TPSpqV8hqQ1JfetxC4GHhNSndpyuN8SavTPvzjUh2/L+lzktZJulnSlLTuFyV9T9LtwO90c79J+qGkpZK+A/zmKPtwb0n/mOrxgKQPA68CbpV0a0rzGUlrUv1b23WipK+U8jlJ0j+0O2Yj1OGp0vbcJun6dFy/JEkVsxl0PIbbX0PKPCZ9Jl6dju+B6fU+SRvS8Z4l6ZZ0PG+RNHMs29dNI3x3BRwFrJV0fNoH96bt2xfYH1iXPs/H9K72PRYRjVmAvYH3AN8BVgFnA/umdQdQfACUnk8rvW8G8EfA94HrKX7J+yqUNxtYmx73Af8feDvwT0A/8HLgYeCVwAnA10vvvRw4s2IZ24DXpzLuBpYDAhYDNwD7AZNS+gXA36fH7wb+Ij0+FFgDHA58Ddgjvf5p4F1pX10PnA98tlT2A6W6LASuTGX3AV8HfqVUx6NSuuuAM9Lj+4Dj0+NLgQe6td+AHwJ/UGEfvh34XOn51PTegdJrB6T/+4HbgCPTdv4rcGBa9zfAW8f42Xwq/X8CsBWYnrb9duBNFfPYeTza7S/gjemzMjOl/zBwXuk4tj4jXwPenR6/F7ght+9uWv8LwF+X6nxcerwPL3729wROA24G7gHObR3X3WVpWgv7MeAs4OyIOC4ilkXEz9K6/wSeBZZJ+l/A0603RcQjEXERMBf4fFpuaFdYRPwQeELS0RRfgnuANwF/GxHbI+Jx4NvArv7i/3tE3B8RO4B1wC1RfELvp/gSTwW+rKJ/+JPAz6f3fRn4H5L2oPgyfgE4EfhFYLWke9PzV0fEMmBf4LeBD4xQj4Wl7VwLHAbMKdXx3vT4bmC2pKkUP4zfTq9fDV3fb39XIc39wAJJl0j65YjYOkyaUyWtTXX5eWBu2sdXA2dImgYcC9xYobx27oqIjel43ktxDDs12v46nOKH9a0R8XB6bTnFDzMUn4Wr0uNjKX6IoNjWN42hLt0w2ncXikZUa9+vAv5c0rkUn69tABHxXERcGxELKRozC4BNkl5V32b0VtMC9inAo8BX0p/Ks1or0kGdD/w98BsM6QuTNJ+itfmXFIHuDyuWuQw4k6J10Gr5Dmcbg/fnXhXzB3iu9HhH6fkOYBJwEXBrRBwBvLWVd0Q8TdEKWwycSvHFFPDFiDgqLa+LiI+oOAE3PeW7zwj1EPCnpfe+NiI+P0wdt6d6CRhpMJpu7bf/GuF9O0XEv1H8SN0P/KmkpeX1kg6h+JE6MSKOBP6xVM5VwBnA6cCXW8FhFw23rzo1WjfKYxSNk6NbL0TEI8Djkn4VeAMj//D0avCgEb+7yUKKljMRcTFFC3wKcIekw1qJJB0k6f0UrfB+4J3A4zXUPwuNCtgRcXNEvIOilbAV+Kqkb6Y+1n2AqRGxkuKE01EAkhZKug/4KMWfwnMj4ryIWFex2K9Q/PofA9wE/DNFv3h/6jP8FeAu4EfAXEl7ppbniV3abCha2I+mx2cOWbcM+BSwOiJ+AtwCnCLpIABJB6QvxyXAl4ClwOfSe39G0epuuQl4b9qXSDq4lc9wIuKnwFZJrVbb/y6trm2/pRbW0xFxDfBnFH9el7dtP4rAv1XSy4GTS9uwCdgEfIjiL5ReKtd5pP0F8FPg14E/kXRC6f3LgGuA6yJie3rtuxTdCFAcn++MX/VH1ua7O5Wi2+MJAEmvSX9xXkLRzXeYpKmSbqDYL1OAt0TEr0fEP5S2dcIbyy9/z6UDexlwWWo5b6f4oH9V0l4UrZPfT8mfoPjT8UdjLOv5dOLqpxGxPZ2kOhb4HkVr5Q8i4j8AJF1H0af7A4o/vbvl48AXJb0P+NaQ+t0t6T9JfwJHxHpJHwJultQHvAC8jyJwHpe24e2S3hMRV6k4UfkAcGNEnC/pcOD2dJ7sKYrW52hfiPcAyyU9TRGYW/Wqc7+9HrhU0o60vf8nlXWjpMci4s2S7qHobnqI4k/usi9R9GOvH0PZXRMRT5SPB8U+GbS/Wq3NiHhc0lsptvG9EXEnsILic3BVKdtzKY7P+cBmiuPVMUkrKbozNo11+1K9h/vungR8s5TsPElvTuvWU+yLvSgaJremrqzdknbjba8kBb21wG9GxA96XZ+hUuvyNuCw1Geahdz3W5mky4F7St0/jSRpHvDJiPjlXtelE5KWAcsi4o5e1yV3jeoSqZukucAGipOA2QUdSe8C7gQ+mFmwznq/lUm6m+KKkWt6XZddIekCivM3Vc/NZCMiznawrsYtbDOzhnAL28ysIRywzcwaYkIGbElLJlpZE3GbJmpZE3GbJnJZ40XSckk/1ggDoqnwKRXDCNwn6Rfa5TkhAzZQ58Guq6yJuE0TtayJuE0Tuazx8gWKexFGcjLFncRzKLb3M+0ynKgB28yspyLin4GfjJJkMcX4KZGukpkm6ZWj5Zn1VSIDAwMxa9bsjt+3ectmDhzoaGC+MaurrIm4TRO1rLGUc8/3H26faBix7Rk0aUpH7zn68LEN2Jf7sVq79u4tEbFLFezfb1bEtmcqpY1nNq+jGCKg5cqIuLKcRtJsisHNjhj6fhWjVF4cEd9Jz28B/l9ErBmpzKzvdJw1azar7hyx7mYTxv7HnFNbWavuvLy2suo0ZQ+N6W7mstj2DHu+7tRKaZ+994pnI2LeLhQ33Hgxo7agsw7YZmb1Eqi2nuKNFEM/t0ynGNdmRO7DNjNrEdDXX23ZdSuAd6WrRX4J2BoRj432BrewzczKKk8Q1C4b/S3FhBMDkjZSTDKxB0BE/BWwEngLxTAOT1NhYC4HbDOznbrXJRIRp7dZH6Rp9apywDYzK+tSC3s8jEvAlvQR4JcoZhNplXPHcK9FxEfGow5mZh0TdZ507Nh4trBPSzOSkObLO2+E18zMMqHdr4W9K9IYAksAZswc2wX+ZmZj1p0rQMZFdm3/iLgyIuZFxLy67qoyMyukk45Vlh7IroVtZtYzwl0iZmaNsZuedDQza5hab03vmAO2mVmLgP58TzqOV8D+MfDXklozefcB3xjhNTOzfOxufdgR8Wng08OsGu41M7NMuEvErKtee+4NtZV1+0dPrqWcB2/5RC3lWAW7WwvbzKyx3MI2M2sA+dZ0M7PmyPjWdAdsM7OdfNLRzKw53CViZtYAu/F42C8x0sQGnsTAzPLgLpGhRp3EwONhm1lPZXzSMbufEo+HbWY91bq0r93SA+7DNjNrkbtEzMyaw1eJmJk1gxywzczyV8wQ5oBtZpY/CfU5YLeMNLGBmVkW3MJORpnYwKyyJ+74Vm1lHbjfb9RWluXBAdvMrCEcsM3MmkBpyZQDtplZIuQWtplZU/T1+U5HM7NGcAvbzKwJ3IdtZtYcbmEnnsDAzHLWzZOOkhYBlwH9wLKIuHjI+pnAF4FpKc0FEbFytDw9gYGZWUk3bk2X1A9cAZwEbARWS1oREetLyT4EXBcRn5E0F1gJzB4t3+xOh3oCAzPrGRVdIlWWNuYDGyLioYh4HrgWWDwkTQD7pcdTgU3tMnUftplZSQddIgOS1pSeXxkRV6bHBwOPlNZtBN4w5P0fAW6W9LvA3sCCdgU6YJuZlXQQsLdExLyRshnmtRjy/HTgCxHxCUnHAldLOiIidgzzXsAB28xspy6edNwIzCg9n85LuzzOAhYBRMTtkvYCBihGNR1Wdn3YZmY9pYrL6FYDcyQdImkycBqwYkiah4ETASQdDuwFbB4tU7ewzcxa1J1b0yNim6RzgJsoLtlbHhHrJF0IrImIFcD7gc9J+n2K7pIzI2Jot8kgnsDAzKykW9dhp2uqVw55bWnp8XrguE7y9AQG1hU7dozaMOiqVxz/a7WVVZf7H9laW1mvnzG1trIaKd8bHd0lYmZW5lvTzcwaoOJNMT3jgG1mVuKAbWbWEN0YS2S8OGCbmZW4hW1m1gRywDYzawQBGcdrT2BgZvYiXyUylCcwMLNs9WV80jG7wZ88gYGZ9YyKLpEqSy+4D9vMLBF5t7AdsM3MSjLuwnbANjMr80lHM7Mm6GH/dBUO2GZmiVBXJjAYL57AwMysxC3spNMJDAJ4YfuIEwh31R79+f6qjtWO0Wcb6qo6z6zvu++etZVVl23b6ztWNjr3YZuZNYH7sM3MmqEYSyTfiO2AbWZWknG8dsA2MyvznY5mZk3g8bDNzJrB42GXeDxsM8ubx8MeatTxsM3MeinjeJ1fl8igCQxmeAIDM6uR8j7pmN3tfeUJDAYO9AQGZlaf1nXYVZZeyK6FbWbWS+7DNjNriIzjtQO2mVmZW9hmZk3gwZ8G8XjYZpatYgKDfCN21uNhi/rGqa5x6GgOeMO5tZX15F2fqq2suty1dEGvq9B1R8+e1usqWNKXcRM7u8v6JjoHa7O8SdWW9vlokaQHJW2QdMEIaU6VtF7SOkl/0y5P92GbmSXq0uBPkvqBK4CTgI3AakkrImJ9Kc0c4A+B4yLiSUkHtcvXLWwzs5I+VVvamA9siIiHIuJ54Fpg8ZA0vwVcERFPAkTEj9vWrfPNMTObuPr6VGkBBiStKS1LStkcDDxSer4xvVZ2KHCopFWS7pC0qF3d3CViZpaI4kqRirZExLxRshpq6KUNk4A5wAnAdOBfJB3RGhxvOG5hm5mVdKlLZCMwo/R8OrBpmDRfjYgXIuLfgQcpAvjIdetsU8zMJrCKAz9VODG5Gpgj6RBJk4HTgBVD0twAvLkoVgMUXSQPjZapJzAwMyvpxmXYEbFN0jnATUA/sDwi1km6EFgTESvSuoWS1gPbgfMj4onR8s1uAoNB42HP9HjYZlYf0b0bZyJiJbByyGtLS48DeF9aKsmuS6Q8HvaBAx4P28zq1cFVIrXzVSJmZknVuxh7xQHbzKwk57FEHLDNzEryDdcO2GZmg3gCAzOzBiiuEul1LUbmCQzMzFrkCQx26nQCgzrV9VfQT+6sb4zqRZd/t7ayvnHOG2sra8PjT9VW1qumTamlnBe272ifqEumvmyP2sqqc2KQbnGXiJlZA7hLxMysQdzCNjNriHzDtQO2mdlOEvRn3CfigG1mVuIuETOzhsg4Xjtgm5m1CHkskRZPYGBmWfNofS/hCQzMLFs592F7AgMzs0RAv1Rp6QX3YZuZlWR8VZ8DtplZmQO2mVkDFFOE5RuxHbDNzErcwjYza4iMG9iewMDMrEXApIwjticwqNkB88+prawnV19eW1l1eu3L9+l1FcZBf68rMC4yjn0jyrnO7hIxM0sk35puZtYYGcdrB2wzszJfJWJm1gDCExiYmTWD3MI2M2sMZTyro8fDNjNLhFvYQ406HraZWS85YHfAExiYWS/lPPiTJzAwM0sk6O+rtvRCdgHbzKyX+tLdju2WdiQtkvSgpA2SLhgl3SmSQtK8tnXrcFvMzCas1knHKsuo+Uj9wBXAycBc4HRJc4dJty9wLnBnlfo5YJuZlUjVljbmAxsi4qGIeB64Flg8TLqLgI8Dz1apmwO2mdlOoq/iAgxIWlNalpQyOhh4pPR8Y3rtxZKko4EZEfH1qrXzeNhmZonoaPCnLRExUr/zcLnEzpVSH/BJ4MwOqufxsM3MdhJM6s6F2BuBGaXn04FNpef7AkcAt6XLCF8BrJD0tohYM1Km2V2HPeFNntLrGthubtv2aJ+oSyb153tN83A6bGGPZjUwR9IhwKPAacA7WysjYiswsLNc6TbgA6MFa3DANjMbpBsTGETENknnADdRTCe0PCLWSboQWBMRK8aSrwO2mVlJt250jIiVwMohry0dIe0JVfJ0wDYzS0Tel845YJuZtag7XSLjxQHbzCwp7nR0wDYza4R8w7UnMDAzGyTjBrYnMDAze5E8HnYnJC1p3Zu/ecvmXlfHzHYjratEqiy9kF3A9gQGZtZL3RoPezz4pKOZWYvyniLMAdvMLPGNM2ZmDeIWtplZQ+Qbrj2BgZnZTgL63cIueAIDePzbl9RW1r899lRtZR36yn1qK+vG9f9RW1kLDj2olnL6uzNofiV1jlH9zPPbayurWzKO1+4SMTN7kVDGnSIO2GZmJW5hm5k1QHFZX74R2wHbzKxFbmGbmTWGx8M2M2uAYgKDXtdiZA7YZmYlvkok8QQGZpa7jHtE8pvAQNISYAnAjJkz66+dme3Wcm5hZzcwlcfDNrNeafVhV1l6wX3YZmYtPZycoAoHbDOzknzDtQO2mdlORZdIviHbAdvMrCTfcO2AbWY2WMYR2xMYmJmVuEsk8QQG8NOnX6itrDonFajTyXNf0esqNNr+p36+trKevO6s2srqlnzDtbtEzMwGyzhiO2CbmSUi7zsdHbDNzFoyHw87u1vTzcx6SRWXtvlIiyQ9KGmDpAuGWf8+Sesl3SfpFkmz2uXpgG1mtpOQqi2j5iL1A1cAJwNzgdMlzR2S7B5gXkQcCVwPfLxd7RywzcxKpGpLG/OBDRHxUEQ8D1wLLC4niIhbI+Lp9PQOYHq7TB2wzcySqt0hKV4PSFpTWpaUsjoYeKT0fGN6bSRnATe2q58nMDAzK6t+0nFLRMzrIJcYNqF0BjAPOL5dgZ7AwMyspEuX9W0EZpSeTwc2vaQsaQHwQeD4iHiuXabZdYl4AgMz66Uu9WGvBuZIOkTSZOA0YMXgcnQ08FngbRHx4yp183XYZmYtXboOOyK2SToHuAnoB5ZHxDpJFwJrImIFcCmwD/DldNXJwxHxttHydcA2Myvp1p2OEbESWDnktaWlxws6zdMB28wsEXnf6eiAbWZWknG8dsA2Mxsk44jtCQzMzEo8gUHiCQzgoP32rK2sU5evrq2s6957TG1l2a6pc1KBZ57fXltZ3ZJvuHaXiJnZYBlHbAdsM7PEExiYmTVF5hMYOGCbmZVkHK8dsM3MXtR+coJecsA2MyvJOF57PGwzs5aq8zX2SnbjYZuZ9VTGETu78bAlLWlNubN5y+ZeV8fMdjOq+K8XsgvYnsDAzHqpSxMYjAufdDQzaxH0Zdwl4oBtZjZIvhHbAdvMLPEEBmZmDZJxvPZ42GZmZW5hJx4P28xy51vTG2BHRC3l1FQMAP909ddqKyveMzEnMMj4u9sIUyb397oKHcv5kDtgm5klvbzGugoHbDOzEk9gYGbWFPnGawdsM7OyjON1dwK2pNOBV0fEx7qRn5lZb4i+jDuxxzT4k6TJkvYuvbSIEa6nHiatmVmWWnc65jr4U0cBW9Lhkj4BPAgcml4TcBSwVtLxku5Nyz2S9gX2B9ZJ+qykiXntl5lZDdp2iaTW8anAWRQ/QFcBR0bEz1KSo4HvRURI+gDwOxGxStI+wLMR8TNJrwP+J/AxSQemPK6JiJ8MU94SYAnAjJkzd30Lzcw6kHGPSKUW9mMUwfrsiDguIpaVgjUU3SE3psergD+XdC4wLSK2AUTEcxFxbUQsBBYDC4BNkl41tDCPh21mvdT0CQxOAR4FviJpqaRZQ9YvBG4GiIiLgbOBKcAdkg5rJZJ0kKT3A18D+oF3Ao/v+iaYmXVJxf7rbCcwiIibgZsl/RxwBvBVSVsoAvOTwKSIeAJA0msi4n7gfknHAodJegz4InAYcA3wloh4dHw2x8xs7CbM8KopKF8GXCZpPrAdOAn4ZinZeZLenNatp+gq2Qv4FHBrRJ0jaZiZdW7C3ekYEXcBSPowsKz0+u8Ok/w54Ftjqp2ZWc1ybmHv0iS8EXF2RNzRrcqYmfWaKi5t85EWSXpQ0gZJFwyzfk9Jf5fW3ylpdrs8s5s13cysp7oQsSX1A1cAJwNzgdMlzR2S7CzgyYh4LfBJ4JJ2VXPANjNLBPRJlZY25gMbIuKhiHgeuJbikuayxRQXZABcD5yoNrMnZD3409q1d2+Zsod+1Ot6WHsvm3xRr6tgNvSS446tXXv3TVP20EDF5HtJWlN6fmVEXJkeHww8Ulq3EXjDkPfvTBMR2yRtBX4O2DJSgVkH7IjwnTNmVpuIWNSlrIZrKQ+9Sq5KmkHcJWJm1n0bgRml59OBTSOlkTQJmAq8ZLiOMgdsM7PuWw3MkXSIpMnAacCKIWlWAO9Oj08BvtXuXpWsu0TMzJoo9UmfA9xEMRTH8ohYJ+lCYE1ErAA+D1wtaQNFy/q0dvnKNx+amTWDu0TMzBrCAdvMrCEcsM3MGsIB28ysIRywzcwawgHbzKwhHLDNzBrivwETT0AHbqpgsgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Attentionの分布の描画\n",
    "fig,ax=plt.subplots()\n",
    "heatmap=ax.pcolor(attention_seq[:,:len(x_test[text_no])],cmap=plt.cm.Blues,vmax=1)\n",
    "ax.set_xticks(np.arange(len(x_test[text_no]))+0.5,minor=False)\n",
    "ax.set_yticks(np.arange(attention_seq.shape[0])+0.5,minor=False)\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "ax.set_xticklabels([detokenizer_en[i] for i in x_test[text_no]],minor=False)\n",
    "ax.set_yticklabels([detokenizer_ja[i] for i in output_seq[1:]],minor=False)\n",
    "plt.colorbar(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"775pt\" viewBox=\"0.00 0.00 310.50 775.00\" width=\"311pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 771)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-771 306.5,-771 306.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 139865924291496 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>139865924291496</title>\n",
       "<polygon fill=\"none\" points=\"166,-730.5 166,-766.5 291,-766.5 291,-730.5 166,-730.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"228.5\" y=\"-744.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139865924291552 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>139865924291552</title>\n",
       "<polygon fill=\"none\" points=\"154.5,-657.5 154.5,-693.5 302.5,-693.5 302.5,-657.5 154.5,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"228.5\" y=\"-671.8\">embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 139865924291496&#45;&gt;139865924291552 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>139865924291496-&gt;139865924291552</title>\n",
       "<path d=\"M228.5,-730.4551C228.5,-722.3828 228.5,-712.6764 228.5,-703.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"232.0001,-703.5903 228.5,-693.5904 225.0001,-703.5904 232.0001,-703.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139868145781616 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>139868145781616</title>\n",
       "<polygon fill=\"none\" points=\"11,-657.5 11,-693.5 136,-693.5 136,-657.5 11,-657.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"73.5\" y=\"-671.8\">input_2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139865936934056 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>139865936934056</title>\n",
       "<polygon fill=\"none\" points=\"0,-584.5 0,-620.5 161,-620.5 161,-584.5 0,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-598.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 139868145781616&#45;&gt;139865936934056 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>139868145781616-&gt;139865936934056</title>\n",
       "<path d=\"M75.2303,-657.4551C76.0044,-649.3828 76.9351,-639.6764 77.7976,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"81.2947,-630.8788 78.7653,-620.5904 74.3267,-630.2106 81.2947,-630.8788\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865924292224 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>139865924292224</title>\n",
       "<polygon fill=\"none\" points=\"182,-584.5 182,-620.5 267,-620.5 267,-584.5 182,-584.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"224.5\" y=\"-598.8\">lstm: LSTM</text>\n",
       "</g>\n",
       "<!-- 139865924291552&#45;&gt;139865924292224 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>139865924291552-&gt;139865924292224</title>\n",
       "<path d=\"M227.5112,-657.4551C227.0689,-649.3828 226.5371,-639.6764 226.0442,-630.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"229.5332,-630.3839 225.4913,-620.5904 222.5437,-630.7669 229.5332,-630.3839\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865936934616 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>139865936934616</title>\n",
       "<polygon fill=\"none\" points=\"54.5,-511.5 54.5,-547.5 152.5,-547.5 152.5,-511.5 54.5,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103.5\" y=\"-525.8\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 139865936934056&#45;&gt;139865936934616 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>139865936934056-&gt;139865936934616</title>\n",
       "<path d=\"M86.1854,-584.4551C88.7563,-576.2951 91.8534,-566.4652 94.7132,-557.3887\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"98.1334,-558.18 97.8003,-547.5904 91.4569,-556.0764 98.1334,-558.18\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865924292224&#45;&gt;139865936934616 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>139865924292224-&gt;139865936934616</title>\n",
       "<path d=\"M194.5899,-584.4551C178.7375,-574.8912 159.0828,-563.0334 142.0929,-552.7833\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"143.8559,-549.7593 133.4854,-547.5904 140.2398,-555.753 143.8559,-549.7593\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865908079472 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>139865908079472</title>\n",
       "<polygon fill=\"none\" points=\"98.5,-365.5 98.5,-401.5 160.5,-401.5 160.5,-365.5 98.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-379.8\">dot: Dot</text>\n",
       "</g>\n",
       "<!-- 139865924292224&#45;&gt;139865908079472 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>139865924292224-&gt;139865908079472</title>\n",
       "<path d=\"M227.7893,-584.041C232.0862,-555.8614 237.622,-500.7263 224.5,-456.5\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<path d=\"M224.5,-456.5C215.613,-431.0722 191.4073,-412.69 169.6552,-400.7319\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"171.023,-397.4986 160.5349,-395.9986 167.7984,-403.7117 171.023,-397.4986\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865922235696 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>139865922235696</title>\n",
       "<polygon fill=\"none\" points=\"92,-219.5 92,-255.5 167,-255.5 167,-219.5 92,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-233.8\">dot_1: Dot</text>\n",
       "</g>\n",
       "<!-- 139865924292224&#45;&gt;139865922235696 -->\n",
       "<g class=\"edge\" id=\"edge12\">\n",
       "<title>139865924292224-&gt;139865922235696</title>\n",
       "<path d=\"M224.5,-456.5C200.2182,-387.0238 242.8224,-355.4317 205.5,-292 198.2781,-279.726 187.0834,-269.4649 175.486,-261.2783\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"177.2937,-258.2778 167.0211,-255.6847 173.4345,-264.1179 177.2937,-258.2778\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865910219216 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>139865910219216</title>\n",
       "<polygon fill=\"none\" points=\"74,-438.5 74,-474.5 163,-474.5 163,-438.5 74,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"118.5\" y=\"-452.8\">dense: Dense</text>\n",
       "</g>\n",
       "<!-- 139865936934616&#45;&gt;139865910219216 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>139865936934616-&gt;139865910219216</title>\n",
       "<path d=\"M107.2079,-511.4551C108.8846,-503.2951 110.9044,-493.4652 112.7694,-484.3887\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"116.1984,-485.0902 114.7828,-474.5904 109.3416,-483.6812 116.1984,-485.0902\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865922235864 -->\n",
       "<g class=\"node\" id=\"node11\">\n",
       "<title>139865922235864</title>\n",
       "<polygon fill=\"none\" points=\"4,-146.5 4,-182.5 159,-182.5 159,-146.5 4,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-160.8\">concatenate: Concatenate</text>\n",
       "</g>\n",
       "<!-- 139865936934616&#45;&gt;139865922235864 -->\n",
       "<g class=\"edge\" id=\"edge14\">\n",
       "<title>139865936934616-&gt;139865922235864</title>\n",
       "<path d=\"M88.528,-511.4403C68.2346,-485.0688 34.5,-433.6833 34.5,-383.5 34.5,-383.5 34.5,-383.5 34.5,-310.5 34.5,-267.4372 53.2559,-220.518 67.071,-191.7995\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"70.2314,-193.3044 71.5325,-182.7897 63.9584,-190.198 70.2314,-193.3044\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865910219216&#45;&gt;139865908079472 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>139865910219216-&gt;139865908079472</title>\n",
       "<path d=\"M121.2191,-438.4551C122.4355,-430.3828 123.8981,-420.6764 125.2534,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"128.7449,-412.0003 126.7741,-401.5904 121.823,-410.9572 128.7449,-412.0003\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865910221120 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>139865910221120</title>\n",
       "<polygon fill=\"none\" points=\"62.5,-292.5 62.5,-328.5 196.5,-328.5 196.5,-292.5 62.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"129.5\" y=\"-306.8\">activation: Activation</text>\n",
       "</g>\n",
       "<!-- 139865908079472&#45;&gt;139865910221120 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>139865908079472-&gt;139865910221120</title>\n",
       "<path d=\"M129.5,-365.4551C129.5,-357.3828 129.5,-347.6764 129.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"133.0001,-338.5903 129.5,-328.5904 126.0001,-338.5904 133.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865910221120&#45;&gt;139865922235696 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>139865910221120-&gt;139865922235696</title>\n",
       "<path d=\"M129.5,-292.4551C129.5,-284.3828 129.5,-274.6764 129.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"133.0001,-265.5903 129.5,-255.5904 126.0001,-265.5904 133.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865922235696&#45;&gt;139865922235864 -->\n",
       "<g class=\"edge\" id=\"edge13\">\n",
       "<title>139865922235696-&gt;139865922235864</title>\n",
       "<path d=\"M117.6348,-219.4551C112.0386,-210.9441 105.2478,-200.6165 99.0706,-191.2219\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"101.8136,-189.023 93.395,-182.5904 95.9647,-192.8689 101.8136,-189.023\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865905265800 -->\n",
       "<g class=\"node\" id=\"node12\">\n",
       "<title>139865905265800</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-73.5 30.5,-109.5 132.5,-109.5 132.5,-73.5 30.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-87.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 139865922235864&#45;&gt;139865905265800 -->\n",
       "<g class=\"edge\" id=\"edge15\">\n",
       "<title>139865922235864-&gt;139865905265800</title>\n",
       "<path d=\"M81.5,-146.4551C81.5,-138.3828 81.5,-128.6764 81.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-119.5903 81.5,-109.5904 78.0001,-119.5904 85.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 139865900895200 -->\n",
       "<g class=\"node\" id=\"node13\">\n",
       "<title>139865900895200</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-.5 30.5,-36.5 132.5,-36.5 132.5,-.5 30.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 139865905265800&#45;&gt;139865900895200 -->\n",
       "<g class=\"edge\" id=\"edge16\">\n",
       "<title>139865905265800-&gt;139865900895200</title>\n",
       "<path d=\"M81.5,-73.4551C81.5,-65.3828 81.5,-55.6764 81.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"85.0001,-46.5903 81.5,-36.5904 78.0001,-46.5904 85.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot',format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
