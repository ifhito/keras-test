{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensotflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_spilt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(file_path):\n",
    "    tokenizer =Tokenizer(filters=\"\")\n",
    "    whole_texts=[]\n",
    "    for line in open(file_path,encoding='utf-8'):\n",
    "        whole_texts.append(\"<s>\"+line.strip()+\"</s>\")\n",
    "        \n",
    "    tokenizer.fit_on_texts(whole_texts)\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts),tokenizer\n",
    "\n",
    "x_train,tokenizer_en = load_data('data/train.en')\n",
    "y_train,tokenizer_ja = load_data('data/train.ja')\n",
    "\n",
    "en_vocab_size=len(tokenizer_en.word_index)+1\n",
    "ja_vocab_size=len(tokenizer_ja.word_index)+1\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.02,random_state=42)\n",
    "x_train=pad_sequences(x_train,padding='post')\n",
    "y_train=pad_sequences(y_train,padding='post')\n",
    "\n",
    "seqX_len=len(x_train[0])\n",
    "seqY_len=len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.kayers import Input,Embedding,Dence,LSTM\n",
    "\n",
    "emb_dim=256\n",
    "hid_dim=256\n",
    "##符号化器\n",
    "#Inputレイヤー（返り値としてテンソルを受け取る）\n",
    "encoder_inputs=Input(shape=(seqX_len,))#(seqX_len,)でseqX_lenという要素だけ持つTupleとなる\n",
    "\n",
    "#モデルの層構成(手前の層の返り値テンソルを、次の接続したい層に別途引数として与える)\n",
    "encoder_embedded=Embedding(en_vocab_size,emb_dim,mask_zero=True)(encoder_inputs)# shape: (seqX_len,)->(seqX_len, emb_dim)\n",
    "# EmbeddingレイヤーとLSTMレイヤーを接続（+LSTMレイヤーのインスタンス化）\n",
    "#*はunpacedある配列i=[1,2,3]をprint('i[1],i[2],i[3]')とできる(多分、zipと一緒に使ったよね)\n",
    "_,*encoder_states=LSTM(hid_dim,return_state=True)(encoder_embedded)# shape: (seqX_len, emb_dim)->(hid_dim, )\n",
    "# このLSTMレイヤーの出力に関しては下記に補足あり(output(h_t),state_h(隠れ状態),state_c(セル状態)=LSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##複合化器\n",
    "#Inputレイヤー(返り値としてテンソルを受け取る)\n",
    "decoder_inputs=Input(shape=(seqY_len))\n",
    "\n",
    "#モデルの層構成(手前の層の返り値テンソルを、次の接続したい層に別途引数として与える)\n",
    "decoder_embedding=Embedding(ja_vocab_size,emb_dim)\n",
    "decoder_embedded=decoder_embedding(decoder_inputs)\n",
    "#EmbeddingレイヤーとLSTMレイヤーを接続(encoder_statesを初期状態として指定)\n",
    "decoder_lstm=LSTM(hid_dim,return_sequences=True,return_state=True)#あとで参照したいので、レイヤー自体を変数化\n",
    "#(_は今回はoutputはいらないため受け取らない)\n",
    "decoder_outputs,_,_=decoder_lstm(decoder_embedding,initial_state=encoder_states)#shape:(seqY_len,emb_dim)->(seqY_len,hid_dim)\n",
    "#LSTMレイヤーとDenseレイヤーを接続\n",
    "decoder_dense=Dense(ja_vocab_size,activation='softmax')\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "\n",
    "model=Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
    "model.compilr(optimizer='rmsprop',loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#サンプリング用（生成用）のモデルを作成\n",
    "\n",
    "#符号化器(学習時と同じ構成、学習したレイヤーを利用)\n",
    "encoder_model=Model(encoder_inputs,encoder_states)\n",
    "\n",
    "#複合化器\n",
    "decoder_states_inputs=[Input(shape=(hid_dim,)),Input(shape=(hid_dim,))]#decorder_lstmの初期状態指定用(h_t,c_t)\n",
    "\n",
    "\n",
    "decoder_inputs=Input(shape=(1,))\n",
    "decoder_embedded=decoder_embedding(decoder_imputs)#学習済みEmbeddingレイヤーを利用\n",
    "decoder_outputs,*decoder_states=decoder_lstm(decoder_embedding,initial_state=decoder_states_inputs)#学習済みLSTMレイヤーを利用\n",
    "decoder_outputs,*decoder_dense(decoder_outputs)#学習済みDenseレイヤーを利用\n",
    "\n",
    "decoder_model=Model([decoder_input]+decoder_states_inputs,\\\n",
    "                    [decoder_out_puts]+decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq,bos_eos,max_output_length=1000):\n",
    "    states_value=encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq=np.array(bos_eos[0])\n",
    "    output_seq=bos_eos[0][:]\n",
    "    \n",
    "    while True:\n",
    "        output_tokens,*states_value=decoder_model.predict([target_seq]+states_value)\n",
    "        sampled_token_index=[np.argmax(output_tokens[0,-1,:])]\n",
    "        output_seq+=sampled_token_index\n",
    "        \n",
    "        if(sampled_token_index == bos_eos[1] or len(output_seq)>max_output_length):\n",
    "            break\n",
    "            \n",
    "        target_seq=np.array(sampled_token_index)\n",
    "        \n",
    "    return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer_en=dict(map(reversed,tokenizer_en.word_index.items()))\n",
    "detokenizer_ja=dict(map(reversed,tokenizer_ja.word_index.items()))\n",
    "\n",
    "text_no=0\n",
    "input_seq=pad_sequences([x_test[text_no]],seqX_len,padding=\"post\")\n",
    "bos_eos=tokenizer_ja.texts_to_sequences([\"<s>\",\"</s>\"])\n",
    "\n",
    "print('元の文',''.join([detokenizer_en[i] for i in x_test[text_no]]))\n",
    "print('生成文',''.join([detokenizer_ja[i] for i in y_test[text_no]]))\n",
    "print('正解文',''.join([detokenizer_ja[i] for i in y_test[test_no]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detokenizer_en=dict(map(reversed,tokenizer_en.word_index.items()))\n",
    "detokenizer_ja=dict(map(reversed,tokenizer_ja.word_index.items()))\n",
    "\n",
    "text_no=0\n",
    "input_seq=pad_sequences([x_test[text_no]],seqX_len,padding=\"post\")\n",
    "bos_eos=tokenizer_ja.texts_to_sequences([\"<s>\",\"</s>\"])\n",
    "\n",
    "print('元の文',''.join([detokenizer_en[i] for i in x_test[text_no]]))\n",
    "print('生成文',''.join([detokenizer_ja[i] for i in y_test[text_no]]))\n",
    "print('正解文',''.join([detokenizer_ja[i] for i in y_test[test_no]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "\n",
    "prediction=['I','am','a','graduate','student','at','a','university']\n",
    "reference=[['I','am','a','graduate','student','at','the','university','of','tokyo']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_no =1\n",
    "input_seq=pad_sequences([x_test[test_no]],seqX_len,padding='post')\n",
    "bos_eos=tokenizer_ja.texts_to_sequences([\"<s>\",\"</s>\"])\n",
    "\n",
    "prediction=[detokenizer_ja[i] for i in decode_sequence(input_seq,bos_eos)]\n",
    "\n",
    "reference=[[derokenizer_ja[i] for i in y_test[text_no]]]\n",
    "\n",
    "print(prediction)\n",
    "print(reference)\n",
    "\n",
    "print(sentence_bleu(reference,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ここからattention付き'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_data(file_path):#filepathでデータのダウンロード\n",
    "    tokenizer=Tokenizer(filters=\"\")\n",
    "    whole_texts=[]\n",
    "    for line in open(file_path,encoding='utf-8'):\n",
    "        whole_texts.append(\"<s>\"+line.strip()+\"</s>\")#文の始まりと終わりの指定\n",
    "        \n",
    "    tokenizer.fit_on_texts(whole_texts)#今回はバイナリベクトル表現化している\n",
    "    \n",
    "    return tokenizer.texts_to_sequences(whole_texts),tokenizer\n",
    "#読み込みとtokenizerによる数値化\n",
    "x_train,tokenizer_en=load_data('data/train.en')\n",
    "y_train,tokenizer_ja=load_data('data/train.ja')\n",
    "\n",
    "en_vocab_size=len(tokenizer_en.word_index)+1\n",
    "ja_vocab_size=len(tokenizer_ja.word_index)+1\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_train,y_train,test_size=0.02,random_state=42)\n",
    "\n",
    "x_train=pad_sequences(x_train,padding='post')#各シーケンスの後をパディングする\n",
    "y_train=pad_sequences(y_train,padding='post')\n",
    "\n",
    "seqX_len=len(x_train[0])\n",
    "seqY_len=len(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input,Permute,Activation,Embedding,Dense,LSTM,concatenate,dot\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "emb_dim=256\n",
    "hid_dim=256\n",
    "att_dim=256\n",
    "\n",
    "#符号化器\n",
    "encoder_inputs = Input(shape=(seqX_len,))#英語文の一番長い文章の長さでshape\n",
    "#mask_zeroはパディング部分を無視する\n",
    "encoder_embedded=Embedding(en_vocab_size,emb_dim,mask_zero=True)(encoder_imputs)#単語ごとに入力\n",
    "\n",
    "encoded_seq,*encoder_states=LSTM(hid_dim,return_sequences=True,return_state=True)(encoder_embedded)\n",
    "\n",
    "#復号化器\n",
    "decoder_inputs=Input(shape=(seqY_len))#日本語の一番長い文でshape\n",
    "decoder_embedding=Embedding(ja_vocab_size,emb_dim)#あとで参照するためにレイヤー自体の変数化を行う\n",
    "decoder_embedding=decoder_embedding(decider_inputs)\n",
    "decoder_lstm=LSTM(hid_dim,return_sequences=True,return_state=True)\n",
    "decoded_seq,_,_=decoder_lstm(decoder_embedding,initial_state=encoder_states)\n",
    "\n",
    "#Attention\n",
    "score_dense=Dense(hid_dim)#全結合層の変数化\n",
    "score=score_dense(decoded_seq)#デコーダの隠れ層出力を全結合層へ\n",
    "\n",
    "score=dot([score,encoded_seq],axis=(2,2))#エンコーダとの出力との類似度を内積でとる\n",
    "\n",
    "attention=Activation('softmax')(score)#正規化\n",
    "\n",
    "context=dot([attention,encoded_seq],axis=(2,1))#アテンションの値でエンコーダの値を重み付け\n",
    "\n",
    "concat=concatenate([context,decoded_seq],axis=2)#アテンションとデコーダの連結(行で並べる)\n",
    "\n",
    "attention_dense=Dense(att_dim,activation='tanh')#attentionの全結合層\n",
    "attentional=attention_dense(concat)#concatをinput\n",
    "\n",
    "output_dense=Dense(ja_vocab_size,activation='softmax')\n",
    "outputs=output_dense(attentional)#outputはアテンションを適応したもの\n",
    "\n",
    "model=Model([encoder_inputs,decoder_inputs],outputs)\n",
    "model.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_target=np.hstack((y_train[:,1:],np.zeros((len(y_train),1),dtype=np.int32)))\n",
    "\n",
    "model.fit([x_train,y_train],np.expand_dims(train_target,-1),batch_size=128,epochs=10,verbose=2,validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成のフェイズ\n",
    "#学習したエンコーダレイヤーを利用(encoder_inputやencoded_seq,encoder_states)\n",
    "encoder_model=Model(encoder_inputs,[encoded_seq]+encoder_states)\n",
    "#学習したエンコーダに合わせてimputの設定(バッチサイズhid_dim)\n",
    "decoder_states_inputs=[Input(shape=(hid_dim,)),Input(shape=(hid_dim,))]\n",
    "#デコーダのインプットは一次元\n",
    "decoder_inputs=Input(shape=(1,))\n",
    "#学習済みモデルで構築\n",
    "decoder_embedded=decoder_embedding(decoder_inputs)\n",
    "#学習済みモデルで構築(initialstateは内部状態の初期値(今回は(hid_dim,hid_dim)))\n",
    "decoded_seq,*decoder_states=decoder_lstm(decoder_embedded,initial_state=decoder_states_inputs)\n",
    "#inputが第一引数、アウトプットが第二引数\n",
    "decoder_model=Model([decoder_imputs]+decoder_states_inputs,[decoded_seq]+decoder_states)\n",
    "#バッチサイズseqX_len,inputdim=hid_dim\n",
    "encoded_seq_in,decoded_seq_in=Input(shape=(seqX_len,hid_dim)),Input(shape=(1,hid_dim))\n",
    "#学習済みdense層を用いる\n",
    "score=score_dense(decoded_seq_in)\n",
    "#以下学習済みattentionを用いる\n",
    "score=dot([score,encoded_seq_in],axes=(2,2))\n",
    "attention=Activation('softmax')(score)\n",
    "context=dot([attention,encoded_seq_in],axes=(2,1))\n",
    "concat=concatenate([context,decoded_seq_in],axis=2)\n",
    "attentional=attention_dense(concat)\n",
    "attention_outputs=output_dense(attentional)\n",
    "\n",
    "attention_model=Model([encoded_seq_in,decoded_seq_in],[attention_outputs,attention])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq,bos_eos,max_output_length=1000):\n",
    "    #predictで予測できるinput_seqは与えるエンコーダの予測がencoded_seqに入り、\n",
    "    #states_valueには時間ごとの(単語ごと)の隠れこみ層の値が入る\n",
    "    encoded_seq,*states_value=encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq=np.array(bos_eos[0])#bos_eos[0]=\"<s>\"に対応するインデックス\n",
    "    #emptyでランダムな配列にできる\n",
    "    output_seq=np.empty((0,len(input_seq[0])))\n",
    "    \n",
    "    while True:\n",
    "        #デコーダの予測値を格納\n",
    "        decoded_seq,*states_value=decoder_model.predict([target_seq]+states_value)\n",
    "        #アテンションの予測値を格納\n",
    "        output_token,attention=attention_model.predict([encoded_seq,decoded_seq])\n",
    "        #argmaxで今回追加したワードの中でもっとも数値が大きいものを選択(argmax内の処理は以下)\n",
    "        '''\n",
    "        import numpy as np\n",
    "        a=np.array([[[2,3],[4,5]],[[6,7],[8,9]]])\n",
    "        print(a[0,-1,:])\n",
    "        output:[4 5]\n",
    "        '''\n",
    "        sampled_token_index=[np.argmax(output_tokens[0,-1,:])]\n",
    "        #出力に値を追加する\n",
    "        output_seq+=sampled_token_index\n",
    "        #\n",
    "        attention_seq=np.append(attention_seq,attention[0],axis=0)\n",
    "        \n",
    "        if(sampled_token_index == bos_eos[1] or len(output_seq)>max_output_length):\n",
    "            break\n",
    "        #\n",
    "        target_seq=np.array(sampled_token_index)\n",
    "        \n",
    "    return output_seq,attention_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#逆順でソート\n",
    "detokenizer_en=dict(map(reversed,tokenizer_en.word_index.items()))\n",
    "detokenizer_ja=dict(map(reversed,tokenizer_ja.word_index.items()))\n",
    "#一文目を予測している\n",
    "text_no=0\n",
    "#各シーケンスの後をパディングする(一番初めの要素を与える)\n",
    "input_seq=pad_sequences([x_test[text_no]],seqX_len,padding='post')\n",
    "bos_eos=tokenizer_ja.texts_to_sequences([\"<s>\",\"</s>\"])\n",
    "\n",
    "output_seq,attention_seq=decode_sequence(input_seq,bos_eos)\n",
    "\n",
    "print('元の文:', ' '.join([detokenizer_en[i] for i in x_test[text_no]]))\n",
    "print('生成文:', ' '.join([detokenizer_ja[i] for i in output_seq]))\n",
    "print('正解文:', ' '.join([detokenizer_ja[i] for i in y_test[text_no]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "prediction = [detokenizer_ja[i] for i in output_seq]\n",
    "reference = [[detokenizer_ja[i] for i in y_test[text_no]]]\n",
    "print(sentence_bleu(reference, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attentionの分布の描画\n",
    "fig,ax=plt.subplots()\n",
    "heatmap=ax.pcolor(attention_seq[:,:len(x_test[text_no])],cmap=plt.cm.Blues,vmax=1)\n",
    "ax.set_xticks(np.arange(len(x_test[text_no]))+0.5,minor=False)\n",
    "ax.set_yticks(np.arange(attention_seq.shape[0])+0.5,minor=False)\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    "ax.set_xticklabels([detokenizer_en[i] for i in x_test[text_no]],minor=False)\n",
    "ax.set_yticklabels([detokenizer_ja[i] for i in output_seq[1:]],minor=False)\n",
    "plt.colorbar(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot',format='svg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
